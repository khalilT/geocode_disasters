{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7f8a6a-8339-4b4c-a81a-0239004cc06d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'a' from 'utils.functions' (/net/home/kteber/geocode_disasters/src/utils/functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mgetcwd(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../src\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_path  \u001b[38;5;66;03m# Import the get_path function from paths.py\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m a\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_smthg\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'a' from 'utils.functions' (/net/home/kteber/geocode_disasters/src/utils/functions.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../src\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../src\")))\n",
    "\n",
    "from utils.paths import get_path  # Import the get_path function from paths.py\n",
    "from utils.functions import a\n",
    "from utils.functions import print_smthg\n",
    "\n",
    "import utils.constants\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(a)\n",
    "    print_smthg()\n",
    "\n",
    "    # Retrieve path to EM-DAT\n",
    "    emdat_data_path = get_path(\"emdat_path\")\n",
    "    print(f\"EM-DAT Data Path: {emdat_data_path}\")\n",
    "\n",
    "    #Load EM-DAT using pandas\n",
    "    if os.path.exists(emdat_data_path):\n",
    "        df = pd.read_excel(emdat_data_path)\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"File not found at {emdat_data_path}\")\n",
    "    \n",
    "    # Retrieve path to Gaul1\n",
    "    gaul1_data_path = get_path(\"gaul1_path\")\n",
    "    print(f\"GAUL 1 Data Path: {gaul1_data_path}\")\n",
    "    \n",
    "    #Load GAUl1 using pandas\n",
    "    if os.path.exists(gaul1_data_path):\n",
    "        df = gpd.read_file(gaul1_data_path)\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(f\"File not found at {gaul1_data_path}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408bcc69-811a-4d24-ac5a-ba54fa666a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM-DAT Data Path: /net/projects/xaida/raw_data/emdat_data/public_emdat_1990_2023.xlsx\n",
      "          DisNo. Historic Classification Key Disaster Group Disaster Subgroup  \\\n",
      "0  1990-0001-LKA      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "1  1990-0002-TUN      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "2  1990-0003-WSM      Yes    nat-met-sto-tro        Natural    Meteorological   \n",
      "3  1990-0004-FRA      Yes    nat-hyd-mmw-ava        Natural      Hydrological   \n",
      "4  1990-0005-IDN      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "\n",
      "         Disaster Type  Disaster Subtype External IDs Event Name  ISO  ...  \\\n",
      "0                Flood    Riverine flood          NaN        NaN  LKA  ...   \n",
      "1                Flood    Riverine flood          NaN        NaN  TUN  ...   \n",
      "2                Storm  Tropical cyclone          NaN        Ofa  WSM  ...   \n",
      "3  Mass movement (wet)   Avalanche (wet)          NaN        NaN  FRA  ...   \n",
      "4                Flood    Riverine flood          NaN        NaN  IDN  ...   \n",
      "\n",
      "  Reconstruction Costs ('000 US$) Reconstruction Costs, Adjusted ('000 US$)  \\\n",
      "0                             NaN                                       NaN   \n",
      "1                             NaN                                       NaN   \n",
      "2                             NaN                                       NaN   \n",
      "3                             NaN                                       NaN   \n",
      "4                             NaN                                       NaN   \n",
      "\n",
      "  Insured Damage ('000 US$) Insured Damage, Adjusted ('000 US$)  \\\n",
      "0                       NaN                                 NaN   \n",
      "1                       NaN                                 NaN   \n",
      "2                       NaN                                 NaN   \n",
      "3                       NaN                                 NaN   \n",
      "4                       NaN                                 NaN   \n",
      "\n",
      "  Total Damage ('000 US$) Total Damage, Adjusted ('000 US$)        CPI  \\\n",
      "0                     NaN                               NaN  42.880732   \n",
      "1                242800.0                          566222.0  42.880732   \n",
      "2                200000.0                          466410.0  42.880732   \n",
      "3                     NaN                               NaN  42.880732   \n",
      "4                  4800.0                           11194.0  42.880732   \n",
      "\n",
      "  Admin Units  Entry Date  Last Update  \n",
      "0         NaN  2005-12-20   2023-09-25  \n",
      "1         NaN  2006-07-19   2023-09-25  \n",
      "2         NaN  2005-12-20   2023-09-25  \n",
      "3         NaN  2005-12-21   2023-09-25  \n",
      "4         NaN  2006-07-19   2023-09-25  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "EMDATA print line\n",
      "          DisNo. Historic Classification Key Disaster Group Disaster Subgroup  \\\n",
      "0  1990-0001-LKA      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "1  1990-0002-TUN      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "2  1990-0003-WSM      Yes    nat-met-sto-tro        Natural    Meteorological   \n",
      "3  1990-0004-FRA      Yes    nat-hyd-mmw-ava        Natural      Hydrological   \n",
      "4  1990-0005-IDN      Yes    nat-hyd-flo-riv        Natural      Hydrological   \n",
      "\n",
      "         Disaster Type  Disaster Subtype External IDs Event Name  ISO  ...  \\\n",
      "0                Flood    Riverine flood          NaN        NaN  LKA  ...   \n",
      "1                Flood    Riverine flood          NaN        NaN  TUN  ...   \n",
      "2                Storm  Tropical cyclone          NaN        Ofa  WSM  ...   \n",
      "3  Mass movement (wet)   Avalanche (wet)          NaN        NaN  FRA  ...   \n",
      "4                Flood    Riverine flood          NaN        NaN  IDN  ...   \n",
      "\n",
      "  Reconstruction Costs ('000 US$) Reconstruction Costs, Adjusted ('000 US$)  \\\n",
      "0                             NaN                                       NaN   \n",
      "1                             NaN                                       NaN   \n",
      "2                             NaN                                       NaN   \n",
      "3                             NaN                                       NaN   \n",
      "4                             NaN                                       NaN   \n",
      "\n",
      "  Insured Damage ('000 US$) Insured Damage, Adjusted ('000 US$)  \\\n",
      "0                       NaN                                 NaN   \n",
      "1                       NaN                                 NaN   \n",
      "2                       NaN                                 NaN   \n",
      "3                       NaN                                 NaN   \n",
      "4                       NaN                                 NaN   \n",
      "\n",
      "  Total Damage ('000 US$) Total Damage, Adjusted ('000 US$)        CPI  \\\n",
      "0                     NaN                               NaN  42.880732   \n",
      "1                242800.0                          566222.0  42.880732   \n",
      "2                200000.0                          466410.0  42.880732   \n",
      "3                     NaN                               NaN  42.880732   \n",
      "4                  4800.0                           11194.0  42.880732   \n",
      "\n",
      "  Admin Units  Entry Date  Last Update  \n",
      "0         NaN  2005-12-20   2023-09-25  \n",
      "1         NaN  2006-07-19   2023-09-25  \n",
      "2         NaN  2005-12-20   2023-09-25  \n",
      "3         NaN  2005-12-21   2023-09-25  \n",
      "4         NaN  2006-07-19   2023-09-25  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "              DisNo. Individual_Location Location_Before Bracketed  \\\n",
      "0      1990-0001-LKA              ampara          ampara      None   \n",
      "1      1990-0001-LKA             badulla         badulla      None   \n",
      "2      1990-0001-LKA               kandy           kandy      None   \n",
      "3      1990-0001-LKA          kurunegala      kurunegala      None   \n",
      "4      1990-0001-LKA              matale          matale      None   \n",
      "...              ...                 ...             ...       ...   \n",
      "13552  2023-9873-FSM              kosrae          kosrae      None   \n",
      "13553  2023-9873-FSM             pohnpei         pohnpei      None   \n",
      "13554  2023-9873-FSM                 yap             yap      None   \n",
      "13555  2023-9879-BWA            southern        southern      None   \n",
      "13556  2023-9879-BWA          north east      north east      None   \n",
      "\n",
      "          Appended  \n",
      "0          ampara,  \n",
      "1         badulla,  \n",
      "2           kandy,  \n",
      "3      kurunegala,  \n",
      "4          matale,  \n",
      "...            ...  \n",
      "13552      kosrae,  \n",
      "13553     pohnpei,  \n",
      "13554         yap,  \n",
      "13555    southern,  \n",
      "13556  north east,  \n",
      "\n",
      "[13557 rows x 5 columns]\n",
      "MNE\n",
      "              DisNo.  ISO             Location\n",
      "0      1990-0073-USA  USA      shadyside, ohio\n",
      "1      1990-0135-USA  USA              florida\n",
      "2      1990-0187-USA  USA  southern california\n",
      "3      1990-0254-USA  USA           washington\n",
      "4      1990-0254-USA  USA                 ohio\n",
      "...              ...  ...                  ...\n",
      "13507  2023-9873-FSM  FSM               kosrae\n",
      "13508  2023-9873-FSM  FSM              pohnpei\n",
      "13509  2023-9873-FSM  FSM                  yap\n",
      "13510  2023-9879-BWA  BWA             southern\n",
      "13511  2023-9879-BWA  BWA           north east\n",
      "\n",
      "[13512 rows x 3 columns]\n",
      "/net/projects/xaida/database_paper/intermediate_data/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../src\")))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../src\")))\n",
    "\n",
    "from utils.paths import get_path  # Import the get_path function from paths.py\n",
    "import utils.constants as constants\n",
    "import utils.functions as functions\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 1 Read and Apply Location Corrections\n",
    "    ######################################\n",
    "    # Read data and apply corrections\n",
    "    \n",
    "    emdat_data_path = get_path(\"emdat_path\")\n",
    "    print(f\"EM-DAT Data Path: {emdat_data_path}\")\n",
    "\n",
    "    #Load EM-DAT using pandas\n",
    "    if os.path.exists(emdat_data_path):\n",
    "        emdat = pd.read_excel(emdat_data_path)\n",
    "        print(emdat.head())\n",
    "    else:\n",
    "        print(f\"File not found at {emdat_data_path}\")\n",
    "    \n",
    "    emdat[\"Location\"] = emdat[\"Location\"].str.lower()\n",
    "\n",
    "    emdat.loc[emdat.ISO == \"PHL\",\"Location\"] = emdat.loc[emdat.ISO == \"PHL\",\"Location\"].replace(constants.rep_philippines, regex=True)\n",
    "    emdat.loc[emdat.ISO == \"BFA\",\"Location\"] = emdat.loc[emdat.ISO == \"BFA\",\"Location\"].replace(constants.rep_burkina, regex=True)\n",
    "    emdat.loc[emdat.ISO == \"HTI\",\"Location\"] = emdat.loc[emdat.ISO == \"HTI\",\"Location\"].replace(constants.rep_haiti, regex=True)\n",
    "    emdat.loc[emdat.ISO == \"TCD\",\"Location\"] = emdat.loc[emdat.ISO == \"TCD\",\"Location\"].replace(constants.rep_chad, regex=True)\n",
    "\n",
    "    Emdata = emdat.copy()\n",
    "    print(\"EMDATA print line\")\n",
    "    print(Emdata.head())\n",
    "    \n",
    "    # 2 Standardize and Clean Location Data\n",
    "    ######################################\n",
    "    #select locations that do not have admin units, but only admin names, and only for natural events\n",
    "    \n",
    "    Emdata_loconly = Emdata[(Emdata['Admin Units'].isna()) & (Emdata['Location'].notna()) & (Emdata[\"Disaster Group\"] == 'Natural')]\n",
    "    \n",
    "    #This script processes and cleans EM-DAT locations. The goal is to standardize and separate the location information.\n",
    "    #The script performs the following key tasks:\n",
    "\n",
    "    #String Replacement: Replaces words like 'and', 'Between', '&', etc., with commas to standardize the separation between multiple locations.\n",
    "    #Location Splitting: Uses a function `split_and_clean_locations` to split location strings based on commas, semicolons, and parentheses, \n",
    "                         #while handling special cases such as entries containing \"Level 1\".\n",
    "    #Location Name Cleanup: removes certain common terms related to geographic divisions (like 'Province', 'District', 'Region', etc.).\n",
    "    #Further Location Splitting: Uses a function `split_text` to break down entries that contain multiple parenthetical groups or locations\n",
    "                        #into individual components for easier parsing.\n",
    "    #Location Extraction: The `extract_locations` function is used to parse locations that contain brackets or geographic references.\n",
    "    #New Data Creation:A new DataFrame is created with four main columns: 'DisNo.' (event ID), 'Individual_Location', 'Location_Before' \n",
    "                        #(location before parentheses), and 'Bracketed' (location inside parentheses). It combines the cleaned-up \n",
    "                        #'Location_Before' and 'Bracketed' columns to create a final 'Appended' location field, which merges both parts where applicable.\n",
    "\n",
    "    #The final DataFrame presents a standardized and cleaned version of the original location data, making it suitable for further analysis or geocoding operations.\n",
    "\n",
    "    Emdata_loconly.loc[:, 'Location'] = Emdata_loconly['Location'].str.replace(r'\\) and\\b', '),', regex=True)\n",
    "\n",
    "    Emdata_loconly.loc[:,'Location'] = Emdata_loconly['Location'].str.replace(r'\\b(and|Between|&|\\+)\\b', ',', regex=True)\n",
    "    \n",
    "    # 3 Split and Parse Location Strings\n",
    "    ######################################\n",
    "    \n",
    "    expanded_rows = [\n",
    "    [row['DisNo.'], row['Location'], loc]\n",
    "    for _, row in Emdata_loconly.iterrows()\n",
    "    for loc in functions.split_and_clean_locations(row['Location'])\n",
    "    ]\n",
    "    \n",
    "    expanded_df = pd.DataFrame(expanded_rows, columns=['DisNo.', 'Full_Location_List', 'Individual_Location'])\n",
    "    \n",
    "    for term in constants.replace_terms:\n",
    "        expanded_df['Individual_Location'] = expanded_df['Individual_Location'].str.replace(term, \" \", case=False,regex=False)\n",
    "    \n",
    "    expanded_df['Individual_Location'] = expanded_df['Individual_Location'].apply(functions.split_text)\n",
    "    expanded_df = expanded_df.explode('Individual_Location', ignore_index=True)\n",
    "    expanded_df['Individual_Location'] = expanded_df['Individual_Location'].str.replace(r'\\b(and| & |Between| \\+ | \\) and)\\b', ',', regex=True)\n",
    "    \n",
    "    new_data = [\n",
    "    [row['DisNo.'], row['Individual_Location'], loc[0], loc[1]]\n",
    "    for _, row in expanded_df.iterrows()\n",
    "    for loc in functions.extract_locations(row['Individual_Location'])\n",
    "    ]\n",
    "\n",
    "    new_df = pd.DataFrame(new_data, columns=['DisNo.', 'Individual_Location', 'Location_Before', 'Bracketed'])\n",
    "    new_df['Appended'] = new_df['Location_Before'] + ','+ new_df['Bracketed'].apply(lambda x: f\" {x}\" if x else \"\")\n",
    "\n",
    "    print(new_df)\n",
    "    \n",
    "    new_df['Appended'] = new_df['Appended'].apply(functions.remove_str_if_last)\n",
    "    \n",
    "    # remove locations that are only numbers or digits\n",
    "\n",
    "    numeric_rows = new_df[[\"Location_Before\"]].applymap(lambda x: isinstance(x, str) and x.isdigit()).any(axis=1)\n",
    "    new_df = new_df[~numeric_rows]\n",
    "    \n",
    "    # Find locations where the number of characters is 2\n",
    "    one_char_rows = new_df[[\"Individual_Location\"]].applymap(lambda x: isinstance(x, str) and len(x) == 2)\n",
    "    usa_rep_rows = new_df[one_char_rows.Individual_Location][(new_df[one_char_rows.Individual_Location][\"DisNo.\"].str.contains(\"USA\"))]\n",
    "\n",
    "    #apply the correction to the locations in the USA\n",
    "    new_df.loc[usa_rep_rows.index] = new_df.loc[usa_rep_rows.index].replace(constants.rep_us_states, regex=True)\n",
    "    \n",
    "    #extract the ISO column from the disaster number\n",
    "    new_df['ISO'] = new_df['DisNo.'].str[-3:]\n",
    "\n",
    "    #search for and remove uncorrect ISO codes.\n",
    "    #For some events, older ISO codes or iso codes other than iso 3 are used by EM-DAT\n",
    "    #ISO to correct: original iso and replacement\n",
    "    #AZO by PRT\n",
    "    #DFR by DEU\n",
    "    #1346 by MNE\n",
    "    #SCG by SRB\n",
    "    new_df['ISO'].replace('AZO', 'PRT', inplace=True)\n",
    "    new_df['ISO'].replace('DFR', 'DEU', inplace=True)\n",
    "    new_df['ISO'].replace('SCG', 'SRB', inplace=True)\n",
    "    \n",
    "    #event in montenegro that belonged to serbia in the past\n",
    "    new_df.loc[(new_df.ISO == \"SRB\") & (new_df.Bracketed == \"montenegro\"),\"ISO\"] = \"MNE\"\n",
    "    \n",
    "    print(new_df.loc[1346, \"ISO\"])\n",
    "    \n",
    "    #events to delete, as they happened in countries that no longer exist (e.g. Youguslavia), at the scale of entire countries today (e.g. Slovenia)\n",
    "    #'ANT'\n",
    "    #'SUN'\n",
    "    #'YUG'\n",
    "    new_df = new_df[~(new_df.ISO == \"YUG\")]\n",
    "    new_df = new_df[~(new_df.ISO == \"SUN\")]\n",
    "    new_df = new_df[~(new_df.ISO == \"ANT\")]\n",
    "    \n",
    "    # 4 Apply Final Corrections and Export\n",
    "    ######################################\n",
    "    #select USA events\n",
    "    new_df_usa = new_df[new_df.ISO == \"USA\"]\n",
    "    new_df_usa = new_df_usa[[\"DisNo.\",\"ISO\",\"Appended\"]].rename(columns={\"Appended\":\"Location\"})\n",
    "    \n",
    "    #select rest of the world events\n",
    "    new_df_restwolrd = new_df[~(new_df.ISO == \"USA\")]\n",
    "    new_df_restwolrd = new_df_restwolrd[[\"DisNo.\",\"ISO\",\"Appended\"]].rename(columns={\"Appended\":\"Location\"})\n",
    "\n",
    "    #concat togethr\n",
    "    df_locations = pd.concat([new_df_usa, new_df_restwolrd])\n",
    "\n",
    "    df_locations = df_locations[~(df_locations[\"Location\"] == 'nan')]\n",
    "\n",
    "    df_locations = df_locations.reset_index().drop(columns='index')\n",
    "    \n",
    "    print(df_locations)\n",
    "    \n",
    "    intermediate_data_path = get_path(\"intermediate_data_path\")\n",
    "    print(intermediate_data_path)\n",
    "    \n",
    "    df_locations.to_csv(intermediate_data_path+'event_locations_to_geolocate.csv')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab95f2-65f2-4b7b-97b4-155a97d43250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtreme3 python=3.8",
   "language": "python",
   "name": "xtreme3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
